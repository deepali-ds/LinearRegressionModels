{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zUHwUD6zgcn0"
   },
   "source": [
    "### Cloning The main repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hdBdhkQc_7H6",
    "outputId": "a4f47b1e-21e5-4064-dd75-3eab42a481b7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'srcnn'...\n",
      "remote: Enumerating objects: 78, done.\u001b[K\n",
      "remote: Total 78 (delta 0), reused 0 (delta 0), pack-reused 78\u001b[K\n",
      "Unpacking objects: 100% (78/78), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/WarrenGreen/srcnn.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B5rfpwKigmxE"
   },
   "source": [
    "# Basic Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "k2LddttfWUbL"
   },
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "import argparse\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tempfile import mkdtemp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Rc-FAhZhDbW"
   },
   "source": [
    "Changing the directory according to need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2avMUKLLA-u7",
    "outputId": "b3a78d2e-fa95-413d-8f68-e8fb8e9edf75"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/srcnn\n"
     ]
    }
   ],
   "source": [
    "%cd /content/srcnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "z4GdTsacuxLg"
   },
   "outputs": [],
   "source": [
    "#from model import get_model\n",
    "#from preprocess import preprocess_dataset\n",
    "#from util import clean_mkdir, load_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4ORZjQRohOYE"
   },
   "source": [
    "# `model.py`\n",
    "The model used for this paper. Implemented in keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "yFcPm2uu1Udq"
   },
   "outputs": [],
   "source": [
    "from keras import Sequential\n",
    "from keras.layers import Convolution2D\n",
    "\n",
    "\n",
    "def get_model(weights_path=None):\n",
    "    model = Sequential()\n",
    "    model.add(\n",
    "        Convolution2D(\n",
    "            32, 9, activation=\"relu\", input_shape=(400, 400, 3), padding=\"same\"\n",
    "        )\n",
    "    )\n",
    "    model.add(Convolution2D(16, 5, activation=\"relu\", padding=\"same\"))\n",
    "    model.add(Convolution2D(3, 5, activation=\"relu\", padding=\"same\"))\n",
    "    if weights_path and weights_path.is_file() and weights_path.suffix == '.h5':\n",
    "        model.load_weights(weights_path)\n",
    "    model.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"accuracy\"])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VamUH50mhg5P"
   },
   "source": [
    "# `clean_mkdir()`\n",
    "A basic function to empty a directory.\n",
    "\n",
    "\n",
    "As `os.rmdir(path)` needs the directory to be empty I changed it to `shutil.rmtree(path)` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "x36LifOD0Keb"
   },
   "outputs": [],
   "source": [
    "def clean_mkdir(path):\n",
    "    if Path(path).exists():\n",
    "        shutil.rmtree(path)\n",
    "\n",
    "    os.makedirs(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s_Jc19wyjxoQ"
   },
   "source": [
    "# `generate_samples()`\n",
    "The function split up a larger image into correctly sized chucks for the model training and testing.\n",
    "\n",
    "In a sense it creates the cropped input data from the \"RAW\" imput image.\n",
    "\n",
    "The images are cropped by a convolutional approach with a size of `(400 x 400 x 3)`. Each crop is rotated 90°, 4 times for augmentation.\n",
    "\n",
    "`image_path` is a single image and the generated images are stored in the `output_path`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "5TAagMME0-eK"
   },
   "outputs": [],
   "source": [
    "ROWS, COLS, CHANNELS = (400, 400, 3)\n",
    "\n",
    "def generate_samples(image_path, output_path):\n",
    "    \"\"\"\n",
    "    Split up a larger image into correctly sized chucks for the model.\n",
    "\n",
    "    Args:\n",
    "        image_path:\n",
    "        data_path:\n",
    "\n",
    "    \"\"\"\n",
    "    #output_path = Path(data_path) / RAW_PATH\n",
    "    clean_mkdir(output_path)\n",
    "    filename = Path(image_path).stem\n",
    "    img_src = Image.open(image_path)\n",
    "\n",
    "    rows = img_src.height\n",
    "    cols = img_src.width\n",
    "    \n",
    "    count = 0\n",
    "    tot = len(range(0, cols - COLS - 1, COLS // 2))*len(range(0, rows - ROWS - 1, ROWS // 2))*4\n",
    "    with tqdm(range(tot)) as pbar:\n",
    "        # iterate starting X\n",
    "        for i in range(0, cols - COLS - 1, COLS // 2):\n",
    "            # iterate starting Y\n",
    "            for j in range(0, rows - ROWS - 1, ROWS // 2):\n",
    "                img_out = img_src.crop((i, j, i + ROWS, j + COLS))\n",
    "\n",
    "                for k in range(4):\n",
    "                    pbar.n = count+1\n",
    "                    pbar.set_description(f\"Generating Samples for {image_path.stem}: \")\n",
    "                    \n",
    "                    img_out_ = img_out.rotate(90*k)\n",
    "                    #print(img_out_.height,img_out_.width)\n",
    "                    img_out_.save(f\"{output_path}/{filename}_{count:05d}.jpg\")\n",
    "                    count += 1\n",
    "\n",
    "    \n",
    "    #print(count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PgNfa76alypG"
   },
   "source": [
    "# `split_sets()`\n",
    "\n",
    "The function takes the raw images directory and divides the images inside to the train_path and test_path directory according to the given fraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "83C0KeFn521D"
   },
   "outputs": [],
   "source": [
    "def split_sets(input_path: Path, train_path: Path, test_path: Path, train_frac = 0.8):\n",
    "    #input_path = data_path / RAW_PATH\n",
    "\n",
    "    #train_path = data_path / TRAIN_PATH\n",
    "    #test_path = data_path / TEST_PATH\n",
    "\n",
    "    clean_mkdir(train_path)\n",
    "    clean_mkdir(test_path)\n",
    "\n",
    "    filenames = []\n",
    "    for filename in os.listdir(input_path):\n",
    "        if filename.endswith(\".jpg\"):\n",
    "            filenames.append(filename)\n",
    "\n",
    "    train_filenames, test_filenames = train_test_split(\n",
    "        filenames, train_size=train_frac, test_size=1-train_frac\n",
    "    )\n",
    "\n",
    "    for filename in train_filenames:\n",
    "        shutil.copyfile(input_path / filename, train_path / filename)\n",
    "\n",
    "    for filename in test_filenames:\n",
    "        shutil.copyfile(input_path / filename, test_path / filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z0hov89EmwJ6"
   },
   "source": [
    "# `generate_dirty()`\n",
    "This function creates the blurred input image that will be fed to the model. The function takes the `input_data_path` and downsamples all the images inside it and saves them in the `output_data_path`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "_s1jPAip6Xiw"
   },
   "outputs": [],
   "source": [
    "def generate_dirty(input_data_path: Path, output_data_path: Path):\n",
    "    \"\"\"\n",
    "    Generate the X values by downsampling clean imagery.\n",
    "\n",
    "    Args:\n",
    "        data_path:\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    clean_mkdir(output_data_path)\n",
    "\n",
    "    for file_ in tqdm(os.listdir(input_data_path), desc = f'Generating Blur data for {output_data_path.stem}'):\n",
    "        img = Image.open(input_data_path / file_)\n",
    "        temp = img.resize((ROWS // 2, COLS // 2), Image.BILINEAR)\n",
    "        temp = temp.resize((ROWS, COLS), Image.BILINEAR)\n",
    "        temp.save(output_data_path / file_)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yD0Qw6qKoIm-"
   },
   "source": [
    "# Constant Paths and the `preprocess_dataset()` function\n",
    "\n",
    "RAW_PATH holds the cropped images generated from the images present in IMAGE_PATH via the `generate_samples()` function.\n",
    "\n",
    "The cropped images present in the RAW_PATH are sorted into the TRAIN_LABEL and TEST_LABEL via the `split_set()` function.\n",
    "\n",
    "The TRAIN_LABEL path and TEST_LABEL path are passed to `generate_dirty()` function to create the input image for the model in the TRAIN and TEST path.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "dPTzUgjuiUVK"
   },
   "outputs": [],
   "source": [
    "IMAGES_PATH = \"images\"\n",
    "\n",
    "RAW_PATH = \"raw\"\n",
    "\n",
    "TRAIN_LABEL = \"train_labels\"\n",
    "TEST_LABEL = \"test_labels\"\n",
    "\n",
    "TRAIN = 'train'\n",
    "TEST = 'test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "ymFrBJFHzo38"
   },
   "outputs": [],
   "source": [
    "def preprocess_dataset(data_path):\n",
    "    data_path = Path(data_path)\n",
    "\n",
    "    images_path = data_path / IMAGES_PATH\n",
    "    raw_path = data_path/RAW_PATH\n",
    "\n",
    "    for filename in os.listdir(images_path):\n",
    "        generate_samples(images_path / filename, raw_path)\n",
    "    print('generate_samples Done...')\n",
    "\n",
    "    train_path = data_path/TRAIN_LABEL\n",
    "    test_path = data_path/TEST_LABEL\n",
    "    split_sets(raw_path,train_path,test_path)\n",
    "\n",
    "    train_dirty = data_path/TRAIN\n",
    "    test_dirty = data_path/TEST\n",
    "    generate_dirty(train_path,train_dirty)\n",
    "    generate_dirty(test_path,test_dirty)\n",
    "    print('generate_dirty Done...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LudOHRgBqaSZ"
   },
   "source": [
    "# `memap_array_init()`\n",
    "\n",
    "Creates `np.memmap()` array that occupies disk space instead of RAM.\n",
    "\n",
    "The function needs the file name, array shape and array data type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "1-mpYN-VirmB"
   },
   "outputs": [],
   "source": [
    "def memap_array_init(file_name,shape,dtype):\n",
    "    file_ = Path(mkdtemp())/f'{file_name}.npy'\n",
    "    np.save(file_,np.array([]))\n",
    "    temp_array = np.memmap(file_, shape=shape, dtype=dtype)\n",
    "    #temp_array = np.zeros_like(temp_array)\n",
    "\n",
    "    return temp_array\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_a0lvnvVq96q"
   },
   "source": [
    "# `load_data()`\n",
    "Given the `x_path` and `y_path` for input and label for the model the function creates 2 memap array and loads them up in the array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "_-mpFbxXAyGI"
   },
   "outputs": [],
   "source": [
    "def load_data(x_path, y_path=None):\n",
    "    x_files = os.listdir(x_path)\n",
    "    x_files.sort()\n",
    "\n",
    "    x_array = memap_array_init(file_name = 'x_file',shape = (len(x_files), ROWS, COLS, CHANNELS), dtype ='float32')\n",
    "    y_array = memap_array_init(file_name = 'y_file',shape = (len(x_files), ROWS, COLS, CHANNELS), dtype ='float32')\n",
    "\n",
    "    for index, file_ in enumerate(tqdm(x_files, desc=f'Loading {x_path.stem} Data:')):\n",
    "        img = Image.open(x_path / file_)\n",
    "        img_array = np.asarray(img, dtype=\"float32\")\n",
    "        img_array = img_array / 255.0\n",
    "        x_array[index] = img_array\n",
    "\n",
    "        if y_path:\n",
    "            img = Image.open(y_path / file_)\n",
    "            img_array = np.asarray(img, dtype=\"float32\")\n",
    "            img_array = img_array / 255.0\n",
    "            y_array[index] = img_array\n",
    "\n",
    "    return x_array, y_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HSly2-Xorne4"
   },
   "source": [
    "# Train function\n",
    "\n",
    "Given the data path it first preprocesses the data via `preprocess_dataset()` function.\n",
    "\n",
    "Next Loads the Model and checkpoints and the x,y data for training via `load_data()` function.\n",
    "\n",
    "Then it fits the model for the given epoch and saves the model in the given path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "snV8ADPvuaex"
   },
   "outputs": [],
   "source": [
    "def train(data_path, model_path, epochs=10, batch_size=32):\n",
    "    preprocess_dataset(data_path)\n",
    "    print('preprocess_dataset Done...')\n",
    "\n",
    "    clean_mkdir(\"checkpoints\")\n",
    "    checkpointer = ModelCheckpoint(\n",
    "        filepath=\"checkpoints/weights.h5\", verbose=1, save_best_only=True\n",
    "    )\n",
    "    model = get_model(model_path)\n",
    "    print('Model Loaded...')\n",
    "\n",
    "    train_path = data_path / TRAIN\n",
    "    train_labels_path = data_path / TRAIN_LABEL\n",
    "    x, y = load_data(train_path, train_labels_path)\n",
    "    print('load_data Done...')\n",
    "\n",
    "    model.fit(\n",
    "        x,\n",
    "        y,\n",
    "        batch_size=batch_size,\n",
    "        epochs=epochs,\n",
    "        validation_split=0.2,\n",
    "        callbacks=[checkpointer],\n",
    "        shuffle=True,\n",
    "    )\n",
    "\n",
    "    model.save(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "18dLROUWsxH4"
   },
   "source": [
    "# Test function\n",
    "\n",
    "Basic test function to evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "sxZ1lHheu6G4"
   },
   "outputs": [],
   "source": [
    "def test(data_path, model_weights_path):\n",
    "    test_path = data_path / TEST\n",
    "    test_labels_path = data_path / TEST_LABEL\n",
    "    model = get_model(model_weights_path)\n",
    "    x, y = load_data(test_path, test_labels_path)\n",
    "    score = model.evaluate(x, y)\n",
    "    print(model.metrics_names, score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZGjjRsy8s8mo"
   },
   "source": [
    "# `run()`\n",
    "\n",
    "This function takes all the images inside the `input_path` and loads them in x via the `load_data()` function. The model weights are loaded in the model from `model_weights_path`.\n",
    "\n",
    "Next each image from the x array is passed one by one to the model and the output np array is converted to an image and saved to the `output_path`.\n",
    "\n",
    "Here I passes each image one by one because passing the entire x array all at once to the model resulted in the entire RAM useage and thus a burnout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "xs3eWgDTu8Zn"
   },
   "outputs": [],
   "source": [
    "def run(input_path, model_weights_path, output_path):\n",
    "    output_path = Path(output_path)\n",
    "    clean_mkdir(output_path)\n",
    "    #test_path = data_path / TEST\n",
    "    model = get_model(model_weights_path)\n",
    "    x, _ = load_data(input_path)\n",
    "\n",
    "    in_array = np.empty((1,x.shape[1],x.shape[2],x.shape[3]))\n",
    "    out_array = np.empty(in_array.shape)\n",
    "\n",
    "    for index in tqdm(range(x.shape[0])):\n",
    "        in_array[0] = x[index]\n",
    "        out_array = model.predict(in_array).clip(0,1)\n",
    "\n",
    "        out_img = Image.fromarray(np.uint8(out_array[0] * 255))\n",
    "        out_img.save(output_path / f\"{index}.jpg\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QgtYl9i-vG37"
   },
   "source": [
    "# The argument structure according to the Github repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XA2_eaAtvFPn",
    "outputId": "565f9946-d351-4454-bf07-693261b40348"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--epochs'], dest='epochs', nargs=None, const=None, default=10, type=<class 'int'>, choices=None, help=None, metavar=None)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(description=\"Train/evaluate/run SRCNN models\")\n",
    "\n",
    "parser.add_argument(\n",
    "    \"--action\", type=str,\n",
    "    default=\"test\",\n",
    "    help=\"Train or test the model.\",\n",
    "    choices={\"train\", \"test\", \"run\"},\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--model_path\", type=str,\n",
    "    help=\"Filepath of a saved model to use for eval or inference or filepath where to save a newly trained model.\",\n",
    "    default='',\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--output_path\", type=str, \n",
    "    help=\"Filepath to output results from run action\",\n",
    "    default='',\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--data_path\",type=str,\n",
    "    help=\"Filepath to data directory. Image data should exist at <data_path>/images\",\n",
    "    default=\"data\",\n",
    ")\n",
    "parser.add_argument(\"--batch_size\", type=int, default=32)\n",
    "parser.add_argument(\"--epochs\", type=int, default=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1ztl9I7lvTi_"
   },
   "source": [
    "### Argument input for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "8tD-aN72y2at"
   },
   "outputs": [],
   "source": [
    "params = Namespace(\n",
    "    action='train', \n",
    "    batch_size=32, \n",
    "    data_path='data', \n",
    "    epochs=10, \n",
    "    model_path='checkpoints/weights.h5', \n",
    "    output_path='output'\n",
    ")\n",
    "\n",
    "params.data_path = Path(params.data_path)\n",
    "params.model_path = Path(params.model_path)\n",
    "params.output_path = Path(params.output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Jhs2nPyty-_W",
    "outputId": "e8226d50-b378-424a-8d8f-850fb5b9f209"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:2800: DecompressionBombWarning: Image size (101832000 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  DecompressionBombWarning,\n",
      "Generating Samples for aerial: : 100%|██████████| 9512/9512 [01:55<00:00, 82.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generate_samples Done...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Blur data for train: 100%|██████████| 7609/7609 [01:42<00:00, 74.56it/s]\n",
      "Generating Blur data for test: 100%|██████████| 1903/1903 [00:25<00:00, 75.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generate_dirty Done...\n",
      "preprocess_dataset Done...\n",
      "Model Loaded...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading train Data:: 100%|██████████| 7609/7609 [04:52<00:00, 25.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load_data Done...\n"
     ]
    }
   ],
   "source": [
    "if params.action == \"train\":\n",
    "    train(params.data_path, params.model_path, params.epochs, params.batch_size)\n",
    "elif params.action == \"test\":\n",
    "    test(params.data_path, params.model_path)\n",
    "elif params.action == \"run\":\n",
    "    run(params.data_path, params.model_path, params.output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xsXfpnHWtLJ8"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yixnBB4PvzHT"
   },
   "source": [
    "### Argument input for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "yn_YypYCx3ve"
   },
   "outputs": [],
   "source": [
    "params = Namespace(\n",
    "    action='test', \n",
    "    batch_size=32, \n",
    "    data_path='data', \n",
    "    epochs=10, \n",
    "    model_path='models/weights.h5', \n",
    "    output_path='outputs'\n",
    ")\n",
    "\n",
    "params.data_path = Path(params.data_path)\n",
    "params.model_path = Path(params.model_path)\n",
    "params.output_path = Path(params.output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bun0pYyuy5is",
    "outputId": "5d0a1e16-0540-46ee-e466-df33d3eedb62"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:2800: DecompressionBombWarning: Image size (101832000 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  DecompressionBombWarning,\n",
      "Generating Samples for aerial: : 100%|██████████| 9512/9512 [01:21<00:00, 116.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generate_samples Done...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Blur data for train: 100%|██████████| 7609/7609 [01:20<00:00, 94.86it/s]\n",
      "Generating Blur data for test: 100%|██████████| 1903/1903 [00:19<00:00, 96.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generate_dirty Done...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Incase the primary preprocess_dataset() is not called durring training.\n",
    "preprocess_dataset(params.data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SirkB9cJx3s9",
    "outputId": "ea38cfe6-b2f4-4ea3-f264-61bd5fa578aa"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading test Data:: 100%|██████████| 1903/1903 [00:31<00:00, 59.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60/60 [==============================] - 24s 162ms/step - loss: 0.0012 - accuracy: 0.9263\n",
      "['loss', 'accuracy'] [0.0012096548452973366, 0.9262673854827881]\n"
     ]
    }
   ],
   "source": [
    "if params.action == \"train\":\n",
    "    train(params.data_path, params.model_path, params.epochs, params.batch_size)\n",
    "elif params.action == \"test\":\n",
    "    test(params.data_path, params.model_path)\n",
    "elif params.action == \"run\":\n",
    "    run(params.data_path, params.model_path, params.output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6YaBSLtav1rv"
   },
   "source": [
    "### Argument input for the final run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "ql73DpFLx3qM"
   },
   "outputs": [],
   "source": [
    "params = Namespace(\n",
    "    action='run', \n",
    "    batch_size=32, \n",
    "    data_path='data/test', #the directory where the input images are\n",
    "    epochs=10, \n",
    "    model_path='models/weights.h5', \n",
    "    output_path='outputs'\n",
    ")\n",
    "\n",
    "params.data_path = Path(params.data_path)\n",
    "params.model_path = Path(params.model_path)\n",
    "params.output_path = Path(params.output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "euYWggK9cW_Y",
    "outputId": "5281ce67-ac1e-4df6-d1ff-cedb221521bc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:2800: DecompressionBombWarning: Image size (101832000 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  DecompressionBombWarning,\n",
      "Generating Samples for aerial: : 100%|██████████| 9512/9512 [01:21<00:00, 116.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generate_samples Done...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Blur data for train: 100%|██████████| 7609/7609 [01:19<00:00, 95.52it/s]\n",
      "Generating Blur data for test: 100%|██████████| 1903/1903 [00:19<00:00, 95.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generate_dirty Done...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Incase the primary preprocess_dataset() is not called durring training.\n",
    "preprocess_dataset(params.data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wbe0QwtBx3nU",
    "outputId": "e8215f4e-a2fb-4392-8b0b-c4e2e1095535"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading test Data:: 100%|██████████| 1903/1903 [00:16<00:00, 113.88it/s]\n",
      "100%|██████████| 1903/1903 [02:19<00:00, 13.67it/s]\n"
     ]
    }
   ],
   "source": [
    "if params.action == \"train\":\n",
    "    train(params.data_path, params.model_path, params.epochs, params.batch_size)\n",
    "elif params.action == \"test\":\n",
    "    test(params.data_path, params.model_path)\n",
    "elif params.action == \"run\":\n",
    "    run(params.data_path, params.model_path, params.output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "346aiB67wEJi"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k1efbeNwwEGT"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rXucPmY7wEDG"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XshvOmskwEAN"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VnLVWW49wD9a"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "diiBtS8HwNhc"
   },
   "source": [
    "# Misc. code durring my debugging. (Ignore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7YxnWVvYx3jX"
   },
   "outputs": [],
   "source": [
    "temp_npy_file = Path(mkdtemp())/'temp_file.npy'\n",
    "np.save(temp_npy_file,np.array([]))\n",
    "temp_array = np.memmap(temp_npy_file, dtype='float64', shape=(1, ROWS, COLS, CHANNELS))\n",
    "img = Image.open('/content/srcnn/data/train_labels/aerial_00000.jpg')\n",
    "img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "paeyd75D9igH",
    "outputId": "701af26a-07f4-4899-88ea-ccd012a80791"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Loaded...\n"
     ]
    }
   ],
   "source": [
    "clean_mkdir(\"checkpoints\")\n",
    "checkpointer = ModelCheckpoint(\n",
    "    filepath=\"checkpoints/weights.h5\", verbose=1, save_best_only=True\n",
    "    #filepath=\"checkpoints/\", verbose=1, save_best_only=True\n",
    ")\n",
    "model = get_model(params.model_path)\n",
    "print('Model Loaded...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-vYB3-3E9hs4",
    "outputId": "c3d0d9ff-80ce-4bd3-822d-2c23c4b8bb50"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading train Data:: 100%|██████████| 7609/7609 [03:36<00:00, 35.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load_data Done...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_path = params.data_path / TRAIN\n",
    "train_labels_path = params.data_path / TRAIN_LABEL\n",
    "x, y = load_data(train_path, train_labels_path)\n",
    "print('load_data Done...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rdY8GdaW2n_L",
    "outputId": "70b201c0-e753-4593-b7a2-9c2d1585f755"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:2800: DecompressionBombWarning: Image size (101832000 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  DecompressionBombWarning,\n",
      "Generating Samples for aerial: : 100%|██████████| 9512/9512 [01:51<00:00, 84.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generate_samples Done...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Blur data for train: 100%|██████████| 7609/7609 [01:40<00:00, 75.40it/s]\n",
      "Generating Blur data for test: 100%|██████████| 1903/1903 [00:25<00:00, 75.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generate_dirty Done...\n",
      "preprocess_dataset Done...\n",
      "Model Loaded...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading train Data:: 100%|██████████| 7609/7609 [05:07<00:00, 24.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load_data Done...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "preprocess_dataset(params.data_path)\n",
    "print('preprocess_dataset Done...')\n",
    "\n",
    "clean_mkdir(\"checkpoints\")\n",
    "checkpointer = ModelCheckpoint(\n",
    "    filepath=\"checkpoints/weights.h5\", verbose=1, save_best_only=True\n",
    "    #filepath=\"checkpoints/\", verbose=1, save_best_only=True\n",
    ")\n",
    "model = get_model(params.model_path)\n",
    "print('Model Loaded...')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Hrl9owbi8jH-",
    "outputId": "f0859302-bd76-4cea-8587-b55c2c3a5211"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "191/191 [==============================] - ETA: 0s - loss: 0.0109 - accuracy: 0.7800 \n",
      "Epoch 1: val_loss improved from inf to 0.00282, saving model to checkpoints/weights.h5\n",
      "191/191 [==============================] - 8744s 46s/step - loss: 0.0109 - accuracy: 0.7800 - val_loss: 0.0028 - val_accuracy: 0.8862\n",
      "Epoch 2/10\n",
      "191/191 [==============================] - ETA: 0s - loss: 0.0024 - accuracy: 0.8901 \n",
      "Epoch 2: val_loss improved from 0.00282 to 0.00204, saving model to checkpoints/weights.h5\n",
      "191/191 [==============================] - 8844s 46s/step - loss: 0.0024 - accuracy: 0.8901 - val_loss: 0.0020 - val_accuracy: 0.8943\n",
      "Epoch 3/10\n",
      "126/191 [==================>...........] - ETA: 48:39 - loss: 0.0019 - accuracy: 0.8966"
     ]
    }
   ],
   "source": [
    "model.fit(\n",
    "    x,\n",
    "    y,\n",
    "    batch_size=params.batch_size,\n",
    "    epochs=params.epochs,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[checkpointer],\n",
    "    shuffle=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qe_jgTCIe7VT"
   },
   "outputs": [],
   "source": [
    "np.asarray(img,dtype=\"float64\")/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WiaxelMDfTDx"
   },
   "outputs": [],
   "source": [
    "temp_array = np.zeros_like(temp_array)\n",
    "temp_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0jmsNvLyflNs"
   },
   "outputs": [],
   "source": [
    "temp_array[0] = np.asarray(img,dtype=\"float64\")/255.0\n",
    "temp_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vZ1E15hnftaA",
    "outputId": "391b34c9-04e1-4f0e-99e6-6717498513f7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float64')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_array.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GMNuPagix3gK",
    "outputId": "35c3e37e-4f33-404a-dfe2-56b2f893238f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float64')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    for index, file_ in enumerate(tqdm(x_files, desc=f'Loading {x_path.stem} Data:')):\n",
    "        img = Image.open(x_path / file_)\n",
    "        img_array = np.asarray(img, dtype=\"float64\")\n",
    "        img_array = img_array / 255.0\n",
    "        x_array[index] = img_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X8s9UnIIx3c6",
    "outputId": "fd110e87-bf3e-4834-ae6f-f072183dc656"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.668967e-02, 1.259000e+03, 3.400727e-02],\n",
       "       [2.097502e-02, 6.093085e-02, 3.420668e-02],\n",
       "       [1.794626e-02, 5.870845e-02, 2.130877e-02]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "axWoWe9Tx3Z0"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J5pFRSkTx3W9"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wQdXMi1S_zc8"
   },
   "outputs": [],
   "source": [
    "data_path = params.data_path\n",
    "epochs = params.epochs\n",
    "batch_size = params.batch_size \n",
    "model_path = params.model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BdeXrYfpAKtH",
    "outputId": "da4b3354-11a8-4f9d-9ea2-0308c7a94173"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:2800: DecompressionBombWarning: Image size (101832000 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  DecompressionBombWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generate_samples Done...\n",
      "generate_dirty Done...\n",
      "preprocess_dataset Done...\n"
     ]
    }
   ],
   "source": [
    "preprocess_dataset(data_path)\n",
    "print('preprocess_dataset Done...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9FAhv4mSAUR5",
    "outputId": "98173409-101d-43eb-9a40-f8d4d5456a00"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Loaded...\n"
     ]
    }
   ],
   "source": [
    "clean_mkdir(\"checkpoints\")\n",
    "checkpointer = ModelCheckpoint(\n",
    "    #filepath=\"checkpoints/weights.h5\", verbose=1, save_best_only=True\n",
    "    filepath=\"checkpoints/weights.h5\", verbose=1, save_best_only=True\n",
    ")\n",
    "model = get_model()\n",
    "print('Model Loaded...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ywoL70z9ANwg",
    "outputId": "81ea08d4-fa63-4e0c-d375-3e199a365a84"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7609\n",
      "load_data Done...\n"
     ]
    }
   ],
   "source": [
    "train_path = data_path / TRAIN\n",
    "train_labels_path = data_path / TRAIN_LABEL\n",
    "\n",
    "x, y = load_data(train_path, train_labels_path)\n",
    "print('load_data Done...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iV6cNpXM_zai",
    "outputId": "5409bf0d-61aa-4743-d827-08dc58d5211e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "  4/191 [..............................] - ETA: 2:17:34 - loss: 8.0802e-04 - accuracy: 0.5010"
     ]
    }
   ],
   "source": [
    "model.fit(\n",
    "    x,\n",
    "    y,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[checkpointer],\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "model.save(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dxqjL37F_zYA"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4vwpRWA6_zVg"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lku22pCAqOZC"
   },
   "outputs": [],
   "source": [
    "x_path = Path('data/train')\n",
    "y_path = Path('data/train_labels')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qVKlxNZ9_9po",
    "outputId": "2adc74bc-b7b6-484c-91ec-7ec0e52b0bf4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7609\n"
     ]
    }
   ],
   "source": [
    "x, y = load_data(x_path, y_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8PhZRbJbtujU"
   },
   "outputs": [],
   "source": [
    "x_files = os.listdir(x_path)\n",
    "\n",
    "x_array = memap_array_init(file_name = 'x_file',shape = (len(x_files), ROWS, COLS, CHANNELS), dtype ='float64')\n",
    "y_array = memap_array_init(file_name = 'y_file',shape = (len(x_files), ROWS, COLS, CHANNELS), dtype ='float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vDEHisEXuQ8y",
    "outputId": "4b4565f9-bed6-4173-93c6-f571a9d3b47e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading train Data:: 100%|██████████| 7609/7609 [04:48<00:00, 26.34it/s]\n"
     ]
    }
   ],
   "source": [
    "for index, file_ in enumerate(tqdm(x_files, desc=f'Loading {x_path.stem} Data:')):\n",
    "    img = Image.open(x_path / file_)\n",
    "    img_array = np.asarray(img, dtype=\"float64\")\n",
    "    img_array = img_array / 255.0\n",
    "    x_array[index] = img_array\n",
    "\n",
    "    if y_path:\n",
    "        img = Image.open(y_path / file_)\n",
    "        img_array = np.asarray(img, dtype=\"float64\")\n",
    "        img_array = img_array / 255.0\n",
    "        y_array[index] = img_array\n",
    "\n",
    "#print(y_array,img_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P9GIWlOjuT1h"
   },
   "outputs": [],
   "source": [
    "return x_array, y_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M4HInkQcttQG"
   },
   "outputs": [],
   "source": [
    "def load_data(x_path, y_path=None):\n",
    "    x_files = os.listdir(x_path)\n",
    "\n",
    "    x_array = memap_array_init(file_name = 'x_file',shape = (len(x_files), ROWS, COLS, CHANNELS), dtype ='float64')\n",
    "    y_array = memap_array_init(file_name = 'y_file',shape = (len(x_files), ROWS, COLS, CHANNELS), dtype ='float64')\n",
    "\n",
    "    for index, file_ in enumerate(tqdm(x_files, desc=f'Loading {x_path.stem} Data:')):\n",
    "        img = Image.open(x_path / file_)\n",
    "        img_array = np.asarray(img, dtype=\"float64\")\n",
    "        img_array = img_array / 255.0\n",
    "        x_array[index] = img_array\n",
    "\n",
    "        if y_path:\n",
    "            img = Image.open(y_path / file_)\n",
    "            img_array = np.asarray(img, dtype=\"float64\")\n",
    "            img_array = img_array / 255.0\n",
    "            y_array[index] = img_array\n",
    "\n",
    "    #print(y_array,img_array)\n",
    "    return x_array, y_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5_GTrdAM88wR"
   },
   "outputs": [],
   "source": [
    "#preprocess_dataset(data_path)\n",
    "print('preprocess_dataset Done...')\n",
    "\n",
    "train_path = data_path / TRAIN\n",
    "train_labels_path = data_path / TRAIN_LABEL\n",
    "clean_mkdir(\"checkpoints\")\n",
    "checkpointer = ModelCheckpoint(\n",
    "    #filepath=\"checkpoints/weights.h5\", verbose=1, save_best_only=True\n",
    "    filepath=\"checkpoints/\", verbose=1, save_best_only=True\n",
    ")\n",
    "model = get_model()\n",
    "print('Model Loaded...')\n",
    "\n",
    "x, y = load_data(train_path, train_labels_path)\n",
    "print('load_data Done...')\n",
    "\n",
    "model.fit(\n",
    "    x,\n",
    "    y,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[checkpointer],\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "model.save(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "36pdkizx6KyZ"
   },
   "outputs": [],
   "source": [
    "clean_mkdir(\"checkpoints\")\n",
    "checkpointer = ModelCheckpoint(\n",
    "    #filepath=\"checkpoints/weights.h5\", verbose=1, save_best_only=True\n",
    "    filepath=\"checkpoints/\", verbose=1, save_best_only=True\n",
    ")\n",
    "model = get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 416
    },
    "id": "no94E9yk4lgC",
    "outputId": "d7d49ee4-c42f-46e7-b8d3-53890c2b0902"
   },
   "outputs": [],
   "source": [
    "#model.fit(\n",
    "        #x,\n",
    "        #y,\n",
    "        #batch_size=32,\n",
    "        #epochs=2,\n",
    "        #validation_split=0.2,\n",
    "        #callbacks=[checkpointer],\n",
    "        #shuffle=True,\n",
    "    #)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oeYJvgHWBXEx",
    "outputId": "da20d17a-0464-4da1-fbff-9f3d09b429e4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7609, 400, 400, 3)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mfdo1FAZukNO"
   },
   "outputs": [],
   "source": [
    "xl = os.listdir(x_path)\n",
    "yl = os.listdir(y_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K-aDQMgZCAIK",
    "outputId": "5c2adb28-df8c-4c9d-8548-5cb0936500ab"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1903"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(xl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KC_QeYTmqYxi",
    "outputId": "1e33cc7a-5312-44e3-a017-46425811733a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7609\n"
     ]
    }
   ],
   "source": [
    "x, y = [], []\n",
    "index = 0\n",
    "print(len(os.listdir(x_path)))\n",
    "files = os.listdir(x_path)\n",
    "for file_ in files[0:6]:\n",
    "    index += 1\n",
    "    img = Image.open(x_path / file_)\n",
    "    img_array = np.asarray(img, dtype=\"uint8\")\n",
    "    img_array = img_array / (MAX_VAL * 1.0)\n",
    "    x.append(img_array)\n",
    "\n",
    "    img = Image.open(y_path / file_)\n",
    "    img_array = np.asarray(img, dtype=\"uint8\")\n",
    "    img_array = img_array / (MAX_VAL * 1.0)\n",
    "    y.append(img_array)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7Yr-fEMZ6S4j",
    "outputId": "55e7fe47-8b78-47c3-b8c5-11b385fa99de"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7609"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(xl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "je8KpJUwuJnf",
    "outputId": "a5c1204e-beee-4e67-f1f9-d2f8c05d29fc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((6, 400, 400, 3), array([], dtype=float64))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(x).shape, np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p5UBF_mZt2OB"
   },
   "outputs": [],
   "source": [
    "def load_data(x_path, y_path=None):\n",
    "    x, y = [], []\n",
    "    index = 0\n",
    "    print(len(os.listdir(x_path)))\n",
    "    for file_ in os.listdir(x_path):\n",
    "        index += 1\n",
    "        img = Image.open(x_path / file_)\n",
    "        img_array = np.asarray(img, dtype=\"uint8\")\n",
    "        img_array = img_array / (MAX_VAL * 1.0)\n",
    "        x.append(img_array)\n",
    "\n",
    "        if y_path is None:\n",
    "            img = Image.open(y_path / file_)\n",
    "            img_array = np.asarray(img, dtype=\"uint8\")\n",
    "            img_array = img_array / (MAX_VAL * 1.0)\n",
    "            y.append(img_array)\n",
    "\n",
    "    return np.array(x), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lQnACLJX9OJw"
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vBCrY0u562uU",
    "outputId": "0a8d5f6c-4996-4ec5-c132-0ee9d95d4e19"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 400, 400, 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.memmap('filename.npy', dtype='uint8', shape=(2000, 400, 400, 3))\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nw0K5P-4T4DC"
   },
   "outputs": [],
   "source": [
    "x = np.ones_like(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HwSvX_cPegkq",
    "outputId": "e8198db5-5c00-4b7e-b01b-b37952203410"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "memmap([[[[1, 1, 1],\n",
       "          [1, 1, 1],\n",
       "          [1, 1, 1],\n",
       "          ...,\n",
       "          [1, 1, 1],\n",
       "          [1, 1, 1],\n",
       "          [1, 1, 1]],\n",
       "\n",
       "         [[1, 1, 1],\n",
       "          [1, 1, 1],\n",
       "          [1, 1, 1],\n",
       "          ...,\n",
       "          [1, 1, 1],\n",
       "          [1, 1, 1],\n",
       "          [1, 1, 1]],\n",
       "\n",
       "         [[1, 1, 1],\n",
       "          [1, 1, 1],\n",
       "          [1, 1, 1],\n",
       "          ...,\n",
       "          [1, 1, 1],\n",
       "          [1, 1, 1],\n",
       "          [1, 1, 1]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[1, 1, 1],\n",
       "          [1, 1, 1],\n",
       "          [1, 1, 1],\n",
       "          ...,\n",
       "          [1, 1, 1],\n",
       "          [1, 1, 1],\n",
       "          [1, 1, 1]],\n",
       "\n",
       "         [[1, 1, 1],\n",
       "          [1, 1, 1],\n",
       "          [1, 1, 1],\n",
       "          ...,\n",
       "          [1, 1, 1],\n",
       "          [1, 1, 1],\n",
       "          [1, 1, 1]],\n",
       "\n",
       "         [[1, 1, 1],\n",
       "          [1, 1, 1],\n",
       "          [1, 1, 1],\n",
       "          ...,\n",
       "          [1, 1, 1],\n",
       "          [1, 1, 1],\n",
       "          [1, 1, 1]]],\n",
       "\n",
       "\n",
       "        [[[1, 1, 1],\n",
       "          [1, 1, 1],\n",
       "          [1, 1, 1],\n",
       "          ...,\n",
       "          [1, 1, 1],\n",
       "          [1, 1, 1],\n",
       "          [1, 1, 1]],\n",
       "\n",
       "         [[1, 1, 1],\n",
       "          [1, 1, 1],\n",
       "          [1, 1, 1],\n",
       "          ...,\n",
       "          [1, 1, 1],\n",
       "          [1, 1, 1],\n",
       "          [1, 1, 1]],\n",
       "\n",
       "         [[1, 1, 1],\n",
       "          [1, 1, 1],\n",
       "          [1, 1, 1],\n",
       "          ...,\n",
       "          [1, 1, 1],\n",
       "          [1, 1, 1],\n",
       "          [1, 1, 1]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[1, 1, 1],\n",
       "          [1, 1, 1],\n",
       "          [1, 1, 1],\n",
       "          ...,\n",
       "          [1, 1, 1],\n",
       "          [1, 1, 1],\n",
       "          [1, 1, 1]],\n",
       "\n",
       "         [[1, 1, 1],\n",
       "          [1, 1, 1],\n",
       "          [1, 1, 1],\n",
       "          ...,\n",
       "          [1, 1, 1],\n",
       "          [1, 1, 1],\n",
       "          [1, 1, 1]],\n",
       "\n",
       "         [[1, 1, 1],\n",
       "          [1, 1, 1],\n",
       "          [1, 1, 1],\n",
       "          ...,\n",
       "          [1, 1, 1],\n",
       "          [1, 1, 1],\n",
       "          [1, 1, 1]]],\n",
       "\n",
       "\n",
       "        [[[1, 1, 1],\n",
       "          [1, 1, 1],\n",
       "          [1, 1, 1],\n",
       "          ...,\n",
       "          [1, 1, 1],\n",
       "          [1, 1, 1],\n",
       "          [1, 1, 1]],\n",
       "\n",
       "         [[1, 1, 1],\n",
       "          [1, 1, 1],\n",
       "          [1, 1, 1],\n",
       "          ...,\n",
       "          [1, 1, 1],\n",
       "          [1, 1, 1],\n",
       "          [1, 1, 1]],\n",
       "\n",
       "         [[1, 1, 1],\n",
       "          [1, 1, 1],\n",
       "          [1, 1, 1],\n",
       "          ...,\n",
       "          [1, 1, 1],\n",
       "          [1, 1, 1],\n",
       "          [1, 1, 1]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[1, 1, 1],\n",
       "          [1, 1, 1],\n",
       "          [1, 1, 1],\n",
       "          ...,\n",
       "          [1, 1, 1],\n",
       "          [1, 1, 1],\n",
       "          [1, 1, 1]],\n",
       "\n",
       "         [[1, 1, 1],\n",
       "          [1, 1, 1],\n",
       "          [1, 1, 1],\n",
       "          ...,\n",
       "          [1, 1, 1],\n",
       "          [1, 1, 1],\n",
       "          [1, 1, 1]],\n",
       "\n",
       "         [[1, 1, 1],\n",
       "          [1, 1, 1],\n",
       "          [1, 1, 1],\n",
       "          ...,\n",
       "          [1, 1, 1],\n",
       "          [1, 1, 1],\n",
       "          [1, 1, 1]]],\n",
       "\n",
       "\n",
       "        ...,\n",
       "\n",
       "\n",
       "        [[[1, 1, 1],\n",
       "          [1, 1, 1],\n",
       "          [1, 1, 1],\n",
       "          ...,\n",
       "          [1, 1, 1],\n",
       "          [1, 1, 1],\n",
       "          [1, 1, 1]],\n",
       "\n",
       "         [[1, 1, 1],\n",
       "          [1, 1, 1],\n",
       "          [1, 1, 1],\n",
       "          ...,\n",
       "          [1, 1, 1],\n",
       "          [1, 1, 1],\n",
       "          [1, 1, 1]],\n",
       "\n",
       "         [[1, 1, 1],\n",
       "          [1, 1, 1],\n",
       "          [1, 1, 1],\n",
       "          ...,\n",
       "          [1, 1, 1],\n",
       "          [1, 1, 1],\n",
       "          [1, 1, 1]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[1, 1, 1],\n",
       "          [1, 1, 1],\n",
       "          [1, 1, 1],\n",
       "          ...,\n",
       "          [1, 1, 1],\n",
       "          [1, 1, 1],\n",
       "          [1, 1, 1]],\n",
       "\n",
       "         [[1, 1, 1],\n",
       "          [1, 1, 1],\n",
       "          [1, 1, 1],\n",
       "          ...,\n",
       "          [1, 1, 1],\n",
       "          [1, 1, 1],\n",
       "          [1, 1, 1]],\n",
       "\n",
       "         [[1, 1, 1],\n",
       "          [1, 1, 1],\n",
       "          [1, 1, 1],\n",
       "          ...,\n",
       "          [1, 1, 1],\n",
       "          [1, 1, 1],\n",
       "          [1, 1, 1]]],\n",
       "\n",
       "\n",
       "        [[[1, 1, 1],\n",
       "          [1, 1, 1],\n",
       "          [1, 1, 1],\n",
       "          ...,\n",
       "          [1, 1, 1],\n",
       "          [1, 1, 1],\n",
       "          [1, 1, 1]],\n",
       "\n",
       "         [[1, 1, 1],\n",
       "          [1, 1, 1],\n",
       "          [1, 1, 1],\n",
       "          ...,\n",
       "          [1, 1, 1],\n",
       "          [1, 1, 1],\n",
       "          [1, 1, 1]],\n",
       "\n",
       "         [[1, 1, 1],\n",
       "          [1, 1, 1],\n",
       "          [1, 1, 1],\n",
       "          ...,\n",
       "          [1, 1, 1],\n",
       "          [1, 1, 1],\n",
       "          [1, 1, 1]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[1, 1, 1],\n",
       "          [1, 1, 1],\n",
       "          [1, 1, 1],\n",
       "          ...,\n",
       "          [1, 1, 1],\n",
       "          [1, 1, 1],\n",
       "          [1, 1, 1]],\n",
       "\n",
       "         [[1, 1, 1],\n",
       "          [1, 1, 1],\n",
       "          [1, 1, 1],\n",
       "          ...,\n",
       "          [1, 1, 1],\n",
       "          [1, 1, 1],\n",
       "          [1, 1, 1]],\n",
       "\n",
       "         [[1, 1, 1],\n",
       "          [1, 1, 1],\n",
       "          [1, 1, 1],\n",
       "          ...,\n",
       "          [1, 1, 1],\n",
       "          [1, 1, 1],\n",
       "          [1, 1, 1]]],\n",
       "\n",
       "\n",
       "        [[[1, 1, 1],\n",
       "          [1, 1, 1],\n",
       "          [1, 1, 1],\n",
       "          ...,\n",
       "          [1, 1, 1],\n",
       "          [1, 1, 1],\n",
       "          [1, 1, 1]],\n",
       "\n",
       "         [[1, 1, 1],\n",
       "          [1, 1, 1],\n",
       "          [1, 1, 1],\n",
       "          ...,\n",
       "          [1, 1, 1],\n",
       "          [1, 1, 1],\n",
       "          [1, 1, 1]],\n",
       "\n",
       "         [[1, 1, 1],\n",
       "          [1, 1, 1],\n",
       "          [1, 1, 1],\n",
       "          ...,\n",
       "          [1, 1, 1],\n",
       "          [1, 1, 1],\n",
       "          [1, 1, 1]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[1, 1, 1],\n",
       "          [1, 1, 1],\n",
       "          [1, 1, 1],\n",
       "          ...,\n",
       "          [1, 1, 1],\n",
       "          [1, 1, 1],\n",
       "          [1, 1, 1]],\n",
       "\n",
       "         [[1, 1, 1],\n",
       "          [1, 1, 1],\n",
       "          [1, 1, 1],\n",
       "          ...,\n",
       "          [1, 1, 1],\n",
       "          [1, 1, 1],\n",
       "          [1, 1, 1]],\n",
       "\n",
       "         [[1, 1, 1],\n",
       "          [1, 1, 1],\n",
       "          [1, 1, 1],\n",
       "          ...,\n",
       "          [1, 1, 1],\n",
       "          [1, 1, 1],\n",
       "          [1, 1, 1]]]], dtype=uint8)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-HBHdJK7Q_wy",
    "outputId": "35ad9a95-d183-4a6e-b4ad-05d7cf7d08f1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11.52"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3000*400*400*3*8/1000000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DnaGUhVo8ZjZ",
    "outputId": "5d4f6abe-eea3-4d7a-a368-64f5ac2ce6b9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3652320000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "`7609*400*400*3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c02pYwq8uab7"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7YiStNF4JEZv",
    "outputId": "a0bd729b-1a1c-4c2b-b3cc-788c26de25a8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/srcnn\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HIVTC6zcV29E",
    "outputId": "d4a86c85-dcc2-40b0-e5d5-124dd86aea1c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:2800: DecompressionBombWarning: Image size (101832000 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  DecompressionBombWarning,\n",
      "Traceback (most recent call last):\n",
      "  File \"srcnn.py\", line 96, in <module>\n",
      "    train(params.data_path, params.epochs, params.batch_size, params.model_path)\n",
      "  File \"srcnn.py\", line 14, in train\n",
      "    preprocess_dataset(data_path)\n",
      "  File \"/content/srcnn/preprocess.py\", line 106, in preprocess_dataset\n",
      "    generate_samples(str(images_path / filename), data_path)\n",
      "  File \"/content/srcnn/preprocess.py\", line 30, in generate_samples\n",
      "    for i in range(0, cols - COLS - 1, COLS / 2):\n",
      "TypeError: 'float' object cannot be interpreted as an integer\n"
     ]
    }
   ],
   "source": [
    "!python srcnn.py --action train --data_path data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3MIHRKWUI3xq",
    "outputId": "28344e3d-7b7f-4e58-975c-702d0cd84fad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-06-15 12:02:51.517730: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "Traceback (most recent call last):\n",
      "  File \"srcnn.py\", line 91, in <module>\n",
      "    test(Path(params.data_path), Path(params.model_path))\n",
      "  File \"srcnn.py\", line 39, in test\n",
      "    x, y = load_data(test_path, test_labels_path)\n",
      "  File \"/content/srcnn/util.py\", line 28, in load_data\n",
      "    for file in os.listdir(x_path):\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'data/test'\n"
     ]
    }
   ],
   "source": [
    "!python srcnn.py --action test --data_path data --model_path models/weights2.h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7aoEqVOdI_FO"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "2L5UYe_3XA4K",
    "outputId": "90655e27-40a9-4489-f638-65cbbe157683"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'aerial'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PurePath('/content/srcnn/data/images/aerial.jpg').stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lQkhW22CXC9G"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "srcnn.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
